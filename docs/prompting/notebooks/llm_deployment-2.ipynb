{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "59c1389d",
      "metadata": {
        "id": "59c1389d"
      },
      "source": [
        "# Week 3 — RAG Assistant (ShopLite)\n",
        "\n",
        "Self-contained Colab: Qwen/Qwen2.5-7B-Instruct + FAISS + Flask + ngrok."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hJyDY1Jm2YQ3",
      "metadata": {
        "id": "hJyDY1Jm2YQ3"
      },
      "outputs": [],
      "source": [
        "#i wanted to show a run of this but i have hit my collab limit "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iKfJWQY0Pd2s",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKfJWQY0Pd2s",
        "outputId": "456abf66-456c-494b-84c7-7f73bd2a635e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Colab one-cell: Qwen 2.5 7B Instruct (local), FAISS RAG, Flask, ngrok\n",
        "# Behavior: refuses off-topic queries + terse two-line answers.\n",
        "\n",
        "# 0) Installs\n",
        "!pip -q install \"transformers>=4.43\" \"accelerate>=0.33\" \"bitsandbytes>=0.43\" \\\n",
        "                 sentencepiece \"sentence-transformers>=2.7\" faiss-cpu pyyaml flask pyngrok requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JroKnAFKPl0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "JroKnAFKPl0f",
        "outputId": "e036e7c7-86a5-451a-82ac-2d4a970ed553"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'faiss'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2437923156.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1) Imports & GPU check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextwrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjsonify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 1) Imports & GPU check\n",
        "import os, json, time, threading, textwrap, yaml, numpy as np, torch, faiss, requests\n",
        "from typing import List, Dict\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# --- Behavior knobs ---\n",
        "TOP_K = 2                   \n",
        "RELEVANCE_THRESHOLD = 0.32   \n",
        "MAX_NEW_TOKENS = 110      \n",
        "MAX_ANSWER_WORDS = 75        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w1yy-U3APqUT",
      "metadata": {
        "id": "w1yy-U3APqUT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 2) Knowledge base (REPLACE with your 15–20 docs in the assignment)\n",
        "KB_DOCS: List[Dict] = [\n",
        "    {\n",
        "        \"id\": \"doc_returns\",\n",
        "        \"title\": \"Returns & Refunds\",\n",
        "        \"content\": (\n",
        "            \"ShopLite offers a 30-day return window from delivery. Items must be unused and in original packaging. \"\n",
        "            \"Exclusions include perishables and final-sale items. Start from Your Orders to get an RMA and label. \"\n",
        "            \"Return shipping is free for defective or mis-shipped items; otherwise a label cost may be deducted. \"\n",
        "            \"Refunds post within 5–10 business days after inspection.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_tracking\",\n",
        "        \"title\": \"Orders & Tracking\",\n",
        "        \"content\": (\n",
        "            \"Statuses: Processing, Shipped, Out for delivery, Delivered. The tracking page shows carrier, latest scan, and ETA. \"\n",
        "            \"If late by >3 business days, contact Support with your order ID. Address changes are possible only before Shipped.\"\n",
        "        ),\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iXfu-Pl3PrKv",
      "metadata": {
        "id": "iXfu-Pl3PrKv"
      },
      "outputs": [],
      "source": [
        "# 3) Chunking helper (≈150–250 words per chunk, 50 words overlap)\n",
        "WORD_MAX, OVERLAP = 220, 50\n",
        "def chunk_text(doc_id: str, title: str, text: str):\n",
        "    words = text.split()\n",
        "    chunks, start = [], 0\n",
        "    while start < len(words):\n",
        "        end = min(len(words), start + WORD_MAX)\n",
        "        chunk = \" \".join(words[start:end])\n",
        "        chunks.append({\"doc_id\": doc_id, \"title\": title, \"text\": chunk})\n",
        "        if end == len(words): break\n",
        "        start = end - OVERLAP\n",
        "    return chunks\n",
        "CHUNKS: List[Dict] = []\n",
        "for d in KB_DOCS: CHUNKS.extend(chunk_text(d[\"id\"], d[\"title\"], d[\"content\"]))\n",
        "print(f\"Docs: {len(KB_DOCS)}, chunks: {len(CHUNKS)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JFaK5rj_PuOS",
      "metadata": {
        "id": "JFaK5rj_PuOS"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# 4) Embeddings + FAISS (cosine via inner product on L2-normalized vectors)\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vecs = embedder.encode([c[\"text\"] for c in CHUNKS], convert_to_numpy=True, show_progress_bar=False)\n",
        "vecs = vecs / (np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12)\n",
        "index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "index.add(vecs.astype(np.float32))\n",
        "print(\"FAISS index built; dim=\", vecs.shape[1])\n",
        "\n",
        "def retrieve(query: str, k: int = TOP_K):\n",
        "    qv = embedder.encode([query], convert_to_numpy=True)\n",
        "    qv = qv / (np.linalg.norm(qv, axis=1, keepdims=True) + 1e-12)\n",
        "    D, I = index.search(qv.astype(np.float32), k)\n",
        "    hits = []\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "        if idx == -1: continue\n",
        "        hits.append({\"score\": float(score), \"title\": CHUNKS[idx][\"title\"], \"text\": CHUNKS[idx][\"text\"]})\n",
        "    return hits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SAT8YSiePw8Q",
      "metadata": {
        "id": "SAT8YSiePw8Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 5) Load Qwen/Qwen2.5-7B-Instruct locally (4-bit so it fits a T4 16GB)\n",
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=dtype,\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nRxlll5aPzmj",
      "metadata": {
        "id": "nRxlll5aPzmj"
      },
      "outputs": [],
      "source": [
        "# 6) Prompt policy (inline YAML) — terse + refuse when not grounded\n",
        "PROMPTS_YAML = \"\"\"\n",
        "base_answer:\n",
        "  role: system\n",
        "  goal: >\n",
        "    You are a ShopLite assistant. Answer ONLY if the provided context snippets\n",
        "    contain the information. If not, refuse.\n",
        "  rules:\n",
        "    - Be terse and precise. If a single fact is asked, reply in ONE sentence.\n",
        "    - Do not add definitions, disclaimers, or extra details unless asked.\n",
        "    - Use only the snippets; do not invent facts.\n",
        "    - Cite sources by document title(s).\n",
        "    - Output exactly two lines:\n",
        "      - \"Answer: <your answer or refusal>\"\n",
        "      - \"Sources: <Title A>; <Title B>\"  # (leave empty if refusing)\n",
        "  format: |\n",
        "    Answer: <your answer>\n",
        "    Sources: <Title A>; <Title B>\n",
        "\n",
        "refusal_message: |\n",
        "  Answer: Sorry—this isn’t in the ShopLite knowledge base. Try a question about orders, returns, shipping, payments, promotions, reviews, account, or support.\n",
        "  Sources:\n",
        "\"\"\"\n",
        "PROMPTS = yaml.safe_load(PROMPTS_YAML)\n",
        "\n",
        "# helper to render mixed-type YAML rules safely\n",
        "def _render_rules(rules):\n",
        "    lines = []\n",
        "    for r in rules:\n",
        "        if isinstance(r, str):\n",
        "            lines.append(r)\n",
        "        elif isinstance(r, dict):\n",
        "            for k, v in r.items():\n",
        "                lines.append(k)\n",
        "                if isinstance(v, list):\n",
        "                    for item in v:\n",
        "                        lines.append(f\"- {item}\")\n",
        "                else:\n",
        "                    lines.append(str(v))\n",
        "        else:\n",
        "            lines.append(str(r))\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def build_chat(query: str, hits: List[Dict]):\n",
        "    rules_text = _render_rules(PROMPTS[\"base_answer\"][\"rules\"])\n",
        "    system = (\n",
        "        PROMPTS[\"base_answer\"][\"goal\"] + \"\\n\" +\n",
        "        rules_text + \"\\n\" +\n",
        "        \"Format:\\n\" + PROMPTS[\"base_answer\"][\"format\"]\n",
        "    )\n",
        "    if not hits:\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": f\"Question: {query}\\nContext: (none)\"}\n",
        "        ]\n",
        "    ctx = \"\\n\\n\".join([f\"Title: {h['title']}\\nSnippet: {h['text']}\" for h in hits])\n",
        "    user = f\"Question: {query}\\n\\nUse these snippets only:\\n---\\n{ctx}\"\n",
        "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_answer(messages: List[Dict], max_new_tokens=MAX_NEW_TOKENS, temperature=0.2):\n",
        "    def _pick_line(lines, prefix):\n",
        "        prefix = prefix.lower()\n",
        "        cands = [ln for ln in lines if ln.lower().startswith(prefix)]\n",
        "        # Prefer the last candidate that does NOT contain angle-bracket placeholders\n",
        "        for ln in reversed(cands):\n",
        "            body = ln.split(\":\", 1)[1] if \":\" in ln else \"\"\n",
        "            if \"<\" not in ln and \">\" not in ln and body.strip():\n",
        "                return ln\n",
        "        # Fallback to the last candidate if none is clean\n",
        "        return cands[-1] if cands else None\n",
        "\n",
        "    # Generate\n",
        "    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "    )\n",
        "    text = tok.decode(out[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Extract lines\n",
        "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
        "    answer_line = _pick_line(lines, \"answer:\") or f\"Answer: {text}\"\n",
        "    sources_line = _pick_line(lines, \"sources:\") or \"Sources: \"\n",
        "\n",
        "    # Trim the answer body to MAX_ANSWER_WORDS\n",
        "    try:\n",
        "        prefix, body = answer_line.split(\":\", 1)\n",
        "        words = body.strip().split()\n",
        "        if len(words) > MAX_ANSWER_WORDS:\n",
        "            body = \" \".join(words[:MAX_ANSWER_WORDS])\n",
        "        answer_line = f\"{prefix}: {body}\"\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return f\"{answer_line}\\n{sources_line}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E8fpqXcJP6Z_",
      "metadata": {
        "id": "E8fpqXcJP6Z_"
      },
      "outputs": [],
      "source": [
        "# 7) Flask API\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return jsonify(status=\"ok\", model=MODEL_ID)\n",
        "\n",
        "@app.post(\"/ping\")\n",
        "def ping():\n",
        "    data = request.get_json(silent=True) or {}\n",
        "    prompt = data.get(\"prompt\", \"Say hello in one sentence.\")\n",
        "    msgs = [{\"role\":\"system\",\"content\":\"You are helpful.\"},{\"role\":\"user\",\"content\":prompt}]\n",
        "    out = generate_answer(msgs, max_new_tokens=80)\n",
        "    return jsonify(output=out)\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "def chat():\n",
        "    data = request.get_json(silent=True) or {}\n",
        "    query = (data.get(\"query\") or \"\").strip()\n",
        "    if not query:\n",
        "        return jsonify(error=\"missing 'query'\"), 400\n",
        "\n",
        "    hits = retrieve(query, k=TOP_K)\n",
        "    top_score = max([h[\"score\"] for h in hits], default=0.0)\n",
        "\n",
        "    # Refuse if off-topic / weak match\n",
        "    if top_score < RELEVANCE_THRESHOLD:\n",
        "        answer = PROMPTS[\"refusal_message\"]\n",
        "        return jsonify(answer=answer, sources=[], confidence=0.0)\n",
        "\n",
        "    messages = build_chat(query, hits)\n",
        "    answer = generate_answer(messages, max_new_tokens=MAX_NEW_TOKENS)\n",
        "\n",
        "    titles = [h[\"title\"] for h in hits]\n",
        "    confidence = round(min(1.0, max(0.0, 0.5 + 0.5 * float(np.mean([h[\"score\"] for h in hits])))), 2)\n",
        "\n",
        "    return jsonify(answer=answer, sources=sorted(set(titles)), confidence=confidence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nlDNle7rQGe7",
      "metadata": {
        "id": "nlDNle7rQGe7"
      },
      "outputs": [],
      "source": [
        "# 8) ngrok (interactive token prompt)\n",
        "NGROK_TOKEN = input(\"Paste your ngrok token (https://dashboard.ngrok.com): \").strip()\n",
        "if NGROK_TOKEN: ngrok.set_auth_token(NGROK_TOKEN)\n",
        "public_tunnel = ngrok.connect(5002, \"http\")\n",
        "PUBLIC_URL = public_tunnel.public_url\n",
        "print(\"Public URL:\", PUBLIC_URL)\n",
        "\n",
        "def run_server():\n",
        "    app.run(host=\"0.0.0.0\", port=5002, debug=False, use_reloader=False)\n",
        "\n",
        "thread = threading.Thread(target=run_server, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "time.sleep(2)\n",
        "print(\"Server started → endpoints: /health  /ping  /chat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa51c89c",
      "metadata": {
        "id": "fa51c89c"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# 9) Smoke test\n",
        "print(\"Health:\", requests.get(f\"{PUBLIC_URL}/health\", timeout=15).status_code)\n",
        "resp = requests.post(f\"{PUBLIC_URL}/chat\", json={\"query\":\"How long is the return window?\"}, timeout=30)\n",
        "print(\"Status:\", resp.status_code)\n",
        "print(textwrap.shorten(resp.text, width=400))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
